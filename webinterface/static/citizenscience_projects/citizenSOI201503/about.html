<ul class="nav nav-tabs">
  <li class="active"><a href="#about" data-toggle="tab">About</a></li>
  <li><a href="#team" data-toggle="tab">Meet the Team</a></li>
</ul>
<div id="myTabContent" class="tab-content">
  <div class="tab-pane well fade active in" id="about">

      <div style="width: 15%; text-align: center; margin: 0; float: right;">
        <img style="width: 60%;" src="/static/images/photos/sirius.jpg" />
        <img style="width: 60%;" src="/static/images/photos/iver.png" />
    </div>
      <div style="width: 85%; margin: 0;">
        Right now a team of robotics engineers and marine scientists are using robots to gather underwater images.
        <a href="http://tracker.marine.acfr.usyd.edu.au/" target="_blank">Click here to see the robots in action!</a>
        <br>
        <br>
        This data is used for many purposes, including change detection, moniotiring biodiverity biodiversity,  environmental imapact assessments, etc...
        In this collaborative process, the machines gather imagery then the scientists identify and label the content in the pictures.
        Once the content in the images is labeled, these labels can be used to tell the computers what the the marked imagery represents - this "teaches" the computers basic visual facts along the lines of: "This pattern of pixels is a fish," "This shape is a coral," or "This texture and color is sand, while this different looking object is kelp."
        The computer processes that "learn" these lessons are called algorithms.
        </p><p>
        These data collecting robots are producing massive amounts imagery: tens of thousands of pictures can be collected by a
        single underwater robot in a day. It would be impossible for scientists to look at every single image and label every
        object within it. So they look at small sub-sets of imagery and lable only subsets of pixels withon the images.
        </p><p>
        These relatively few examples have been used to train computer algorithms to help label more of the data.
        The more pictures that have labeled content, the more accurate the algorithms become.
        This is where you can come in and can be a huge help: Your involvement helps the robots help the scientists.
        By participating in this team, you are advancing marine science, increasing the efficiency of algorithmic learning, and advancing human/robotic interaction.
        Be a part of these breakthroughs and discovery - lend your eyes and a few click of your mouse to help us understand and identity more of our undersea world.
    </div>

  </div>
  <div class="tab-pane well fade" id="team">

        <p>This website has been developed by the Marine Robotics group at the Australian Centre
        for Field Robotics. </p>

  <h2>Core Development Team</h2>
      <p>
    <b><a href="http://www-personal.acfr.usyd.edu.au/ariell.friedman/" target="_blank"> Ariell Friedman</a></b> is the primary architect and software developer.
    Ariell has a PhD in automated interpretation of benthic imagery and has extensive experience in machine learning, computer vision and active learning.
    He also has over a decade of experience in web development and user interface design. He has worked closely
    with members from the marine science community and has experience creating software tools and data products  that
    assist and facilitate marine science.
</p>
<p>
    <b><a href="http://db.acfr.usyd.edu.au/content.php/232.html?personid=115" target="_blank">Stefan Williams</a></b>
    is the project manager.
    Stefan has had over a decade of
    experience working with autonomous underwater vehicles and currently runs the Marine Robotics group at the
    Australian Centre for Field Robotics, which operates the IMOS AUV facility. He has extensive experience conducting
    benthic surveys and with the challenges involved in working with benthic image data.
    He has worked closely with members of the marine science community and has supervised numerous postgraduate
    and postdoctoral research projects in the areas of machine learning.
</p>
<p>
    <b><a href="http://db.acfr.usyd.edu.au/content.php/232.html?personid=362" target="_blank">Lachlan Toohey</a></b> has
    been instrumental in much of backend and API development. Lachlan is a PhD student at the Australian Centre for
    Field Robotics at the University of Sydney, Australia. His research topic concerns algorithms enabling sharing of
    information between robots to better determine their position in environments where communication is limited.
</p>

      <h2>Acknowledgements</h2>
<p>
    Thanks goes to <b>Oscar Pizarro</b> for the many good ideas and insights throughout the development process. Also
    thankyou to <b>Matt Edmunds</b> for his help in picking good labels and datasets and <b>Logan Mock-Bunting</b> helping
    refine some of the wording for this citizen science interface.
</p>
</div>
</div>




